---
title: "EEIA 2016-brief"
author: "Harpo"
date: "11/17/2016"
output: 
  html_document: 
    code_folding: hide
    fig_height: 4
    fig_width: 8
    highlight: espresso
    number_sections: yes
    theme: united
    toc: yes
---
<style type="text/css">

.table {
  display: block;
  font-family: sans-serif;
  -webkit-font-smoothing: antialiased;
  font-size: 115%;
  overflow: auto;
  width: auto;
}
  th {
    background-color: rgb(238, 238, 238);
    color: black;
    font-weight: normal;
    padding: 20px 30px;
    text-align: center;
  }
  td {
    color: black;
    padding: 20px 30px;
  }
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("../MEMS-config.R")
source("../MEMS-evaluation.R")
source("../MEMS-significance.R")
source("../MEMS-viz.R")
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(knitr))
``` 



```{r, echo=FALSE}
##Setting up Auxiliar Functions

imu_mapnames=c()
imu_mapnames[5]="glad_data"
imu_mapnames[6]="glad_xbow1_target"
imu_mapnames[8]="glad_xsns1_target"
imu_mapnames[9]="glad_xtal_target"

create_datafile<-function(filepath,pattern,name){
  results=c()
  lm_files=list.files(filepath,pattern)
  for (file_id in 1:length(lm_files)){
    results_file=read.csv(paste(filepath,lm_files[file_id],sep=""),header=T)
    results_file=cbind(
      rep(unlist(strsplit(lm_files[file_id],".txt"))[1],nrow(results_file)),
      rep(name,nrow(results_file)),
      results_file)
    names(results_file)[1]="imu"
    names(results_file)[2]="model"
    results=rbind(results,cbind(results_file)
    )
  }
  return(results)
}

```

#Motivation
Several approaches based on machine learning techniques have been proposed to increase MEMS inertial sensors' performances. Generally, such approaches  have assumed that errors cannot be explained by linear models, as well as the observed error probability distribution is not pure Gaussian. However, according to  manufacturers the error non-linearity observed in MEMS sensors can be considered in most cases negligible. Therefore, it seems necessary to evaluate linear models in the context of MEMS inertial sensor errors. In particular the application of Time delayed Multiple Linear Regression models. (TD-MLR)

#Main Hypotesis
Simpler Machine learning linear models are capable of  compensating  errors observed in MEMS sensors.

# Models considered

Three models are considered:

1. **Time Delayed - Multi Linear Regression (TD-MLR)** : A simple linear model
2. **Multi Layer Perceptron** (MLP), a non linear model
3. **Moving Average** (MA): the common strategy used in navigation systems for dealing with noise

A brief description of each one of the models is presented in the following subsections.

## Multi Linear Regression Mode (MLR). 

A TD-MLR model for representing the relationship between an explanatory variable $x$ and a response variable $y$ can be written as, 

$$y_t = \alpha  + \beta_t L^q( x_{t})  + \epsilon_{t}$$


\noindent where, the subscript $t$ refers to the $t^{th}$ time unit in the population of size $M  \in \mathbb{N}$. $y_t$ denotes the observed response for experimental time unit $t$. $\epsilon_t$ and $\alpha$ are  respectively the residual error and the intercept terms for the same time unit $t$. The expression $L^q( x_{t})$ represents a  tapped delay line for  the $x_t$ variable. A tapped delay line is a well-known method to model non-stationary time series \cite{Haykin1998} implemented as a short-term memory with $q$ unit delay operators

## Multi Layer Perceptron (MLP)

(From Wikipedia)
A multilateral perceptron (MLP) is a feed forward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called back-propagation for training the network.[1][2] MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable.

## Moving Average (MA)

The usual technique for dealing with correlated noise consists of applying a Moving Average (MA) filter. The MA filter operates by averaging a predefined number of points from the input signal to produce each point in the output signal. As its name implies, a MA is an average that moves along the input signal values. An old value is dropped as a new value comes available. The number of values to keep will depend of the size of the time window considered. More formally, given an input signal $x$  of size $M$ defined as ${x_t}_{(t=1)}^{M}$, a MA with a time window of size $q$ will output a new signal  ${y_t}_{(t=1)}^{M-q}$ defined from the $x_i$ by taking the arithmetic mean of sub-sequences of $q$ terms,

$$y_t = \frac{1}{q} \sum_{s=0}^{M-q+1} x_{t-s}$$

#Experiments

## Dataset Description

The performance of TD-MLR models is evaluated on the well-known Melbourne dataset \cite{Toth2011}, which was kindly provided by research members from The Ohio State University (OSU) and the University of Melbourne. Since its publication in 2011, the Melbourne dataset has become a reference in the research field of MEMS inertial sensors.

The  dataset contains measures provided by a mobile platform that includes various grades of IMUs, several GPS receivers and data-logging software. The IMUs included in the mobile platform varies from low-quality MEMS sensors to high-quality, navigation-grade sensors. The information provided by IMUs were logged during a predefined trajectory performed by a ground vehicle. As shown in  Fig. \ref{fig:trajectory},  the trajectory has two well-defined stretches. 

```{r}
  trajectory1=read.csv("/home/harpo/Dropbox/shared/MEMS-ANN/datasets/dgps_T1.txt",header=F)
  trajectory2=read.csv("/home/harpo/Dropbox/shared/MEMS-ANN/datasets/dgps_T2.txt",header=F)
  par(mfrow=c(1,2))
  plot(trajectory1$V1,trajectory1$V2,type="l",lwd=4,ylab="Latitude [degree]",xlab="Longitude [degree]",main="Stretch 1 (T1)",col="orange",cex=1.2)
  plot(trajectory2$V1,trajectory2$V2,type="l",lwd=4,ylab="Latitude [degree]",xlab="Longitude [degree]",main="Stretch 2 (T2)",col="orange",cex=1.2)
```

In the first stretch (T1, for short) the vehicle performs several 8-pattern loops at various velocities and accelerations in a parking area at OSU for about 16 minutes. Then, in the second stretch (T2) the vehicle is driven on an internal OSU road for an 8 minutes trip. Notice that T1 has more rich dynamics than T2. The complete dataset covers about 24 minutes.


In the present work,  four MEMS IMUs are considered: 

1. Gladiator Landmark 10 (Gladiator).  
2. Xbow IMU400CD (Crossbow) 
3. Xsens (xsns1)
4. Xtal (xtal)

These four IMUs represent MEMS inertial sensors of different grades, Gladiator is a low quality IMU very sensible to noise, while the remaining are mid-range quality IMUS with variable sensor characteristics .
A fifth   Non-MEMS IMU is also considered as reference. Namely, the Honeywell H764G-1 (Honeywell), a well-established navigation-grade IMU in use on most military aircraft 

## Finding the optimal models

TD-MLR and MLP models are built for each one of the available sensors (i.e AccX, AccY, GyroZ). The models are built using the response observations (i.e. $y_t$) from **nav-grade Honeywell  IMU**  while the explanatory variables (i.e. $X_t$) will consist of  time delayed values coming from the corresponding low and mid quality  IMUs sensors (i.e Gladiator, Xbow, Xtal and Xsens). 

```{r, fig.height=6, fig.width=15, message=FALSE, warning=FALSE, include=FALSE}

# Reading all results from CV model sweeping -----------------------------------------
results=c()
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_5.*","mlp5"))
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_10.*","mlp10"))
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_20.*","mlp20"))   
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_30.*","mlp30"))   
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_40.*","mlp40"))   
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_60.*","mlp60"))   
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_80.*","mlp80"))  
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*mlp_.*node_100.*","mlp100"))  
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*lm_.*","mlr"))   
results=rbind(results,create_datafile(paste(results_dir,"/data/",sep=""),"*ma_.*","ma"))   
results=results %>%  mutate(imuid= match(imu,imu_mapnames ))

```
## Plot MLP vs. MLR

```{r}
xy_data_full=results %>% group_by(timedelay,model,imu)%>% summarise(meanX6=mean(X6),meanX1=mean(X1),meanX2=mean(X2))

xy_data_mlp_x1=xy_data_full %>% filter(grepl("mlp",model)) %>% group_by(timedelay,imu) %>% filter(meanX1==min(meanX1))
# rename mlpxxx to mlp
xy_data_mlp_x1$model=factor(xy_data_mlp_x1$model, levels=c(levels(xy_data_mlp_x1$model), "mlp"))
xy_data_mlp_x1[grepl("mlp",xy_data_mlp_x1$model),]$model='mlp'

xy_data_mlp_x2=xy_data_full %>% filter(grepl("mlp",model)) %>% group_by(timedelay,imu) %>% filter(meanX2==min(meanX2))
# rename mlpxxx to mlp
xy_data_mlp_x2$model=factor(xy_data_mlp_x2$model, levels=c(levels(xy_data_mlp_x2$model), "mlp"))
xy_data_mlp_x2[grepl("mlp",xy_data_mlp_x2$model),]$model='mlp'

xy_data_mlp_x6=xy_data_full %>% filter(grepl("mlp",model)) %>% group_by(timedelay,imu) %>% filter(meanX6==min(meanX6))
# rename mlpxxx to mlp
xy_data_mlp_x6$model=factor(xy_data_mlp_x6$model, levels=c(levels(xy_data_mlp_x6$model), "mlp"))
xy_data_mlp_x6[grepl("mlp",xy_data_mlp_x6$model),]$model='mlp'


# Function
# getting the best mlp models for each sensor
get_xydata<-function(xy_data_mlp){
# adding the rest of the models
  xy_data= rbind(xy_data_mlp, xy_data_full %>% filter(model=='mlr' | model=='ma'))
  xyplot(meanX1~timedelay|model+imu,data=xy_data,type='b',layout=c(1,3),auto.key = T,scales=list(relation='free'),strip=FALSE)
  return (xy_data)
}
```

```{r, fig.height=8, fig.width= 8}

# Function
plot_model_performance<-function(mlp_model,sensorName){
  xy_data<-get_xydata(mlp_model)
  xy_data[grepl("mlr",xy_data$model),]$model='td-mlr'
  xy_data$imu=factor(xy_data$imu, levels=c(levels(xy_data$imu), "Gladiator","Crossbow","XSens","Xtal"))
  xy_data[which(xy_data$imu=='glad_data'),]$imu="Gladiator"
  xy_data[which(xy_data$imu=='glad_xbow1_target'),]$imu="Crossbow"
  xy_data[which(xy_data$imu=='glad_xsns1_target'),]$imu="XSens"
  xy_data[which(xy_data$imu=='glad_xtal_target'),]$imu="Xtal"
  library(ggplot2)
  gg=ggplot(xy_data,aes(x=timedelay,y=meanX1,col=model,shape=model) )+
    geom_point(alpha=0.5)+
    geom_line()+
    facet_wrap(~imu,scales = 'free_y',nrow = 1,ncol = 4)+
    theme_minimal()+
    theme(legend.position="bottom")+
    ggtitle(paste0("Model performance on ", sensorName))+
  ylab("Avg RMSE ")+xlab("Lag number")
    #guides(color=FALSE,shape=FALSE)
  return(gg)
}

plot_accx=plot_model_performance(xy_data_mlp_x1,"AccX")
ggsave(plot_accx,filename = "~/Dropbox/shared/MEMS-ANN/results/figs/best_models-performance-x1.pdf",width=9,height=2.5)
plot_accy=plot_model_performance(xy_data_mlp_x2,"AccY")
ggsave(plot_accy,filename = "~/Dropbox/shared/MEMS-ANN/results/figs/best_models-performance-x2.pdf",width=9,height=2.5)

plot_gyroz=plot_model_performance(xy_data_mlp_x6,"GyroZ")
ggsave(plot_gyroz,filename = "~/Dropbox/shared/MEMS-ANN/results/figs/best_models-performance-x6.pdf",width=9,height=2.5)

gridExtra::grid.arrange(plot_accx,plot_accy,plot_gyroz)
```


```{r selecting-best-models}
best_models=data.frame()

# Selecting best models por MLP  using ANOVA -----------------------------------------
imu_results=results %>% filter(imuid==5 & grepl("mlp",model)) %>% select(timedelay,model,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best<-select_n_taps_mlp(imu_results,"Gladiator",c("X1","X2","X6"))[,c(3,4,5,6)]
best_models=rbind(best_models,data.frame(imuid=rep(5,3),model=best$model,sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd))

imu_results=results %>% filter(imuid==6 & grepl("mlp",model)) %>% select(timedelay,model,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps_mlp(imu_results,"Crossbow",c("X1","X2","X6"))[,c(3,4,5,6)]
best_models=rbind(best_models,data.frame(imuid=rep(6,3), model=best$model,sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==8 & grepl("mlp",model)) %>% select(timedelay,model,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps_mlp(imu_results,"Xsens",c("X1","X2","X6"))[,c(3,4,5,6)]
best_models=rbind(best_models,data.frame(imuid=rep(8,3), model=best$model,sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==9 & grepl("mlp",model)) %>% select(timedelay,model,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps_mlp(imu_results,"Xtal",c("X1","X2","X6"))[,c(3,4,5,6)]
best_models=rbind(best_models,data.frame(imuid=rep(9,3), model=best$model,sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

# Selecting best models por MLR  using ANOVA ---------------------------------------
imu_results=results %>% filter(imuid==5 & model=='mlr') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Gladiator",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(5,3),model=rep('mlr',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==6 & model=='mlr') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Crossbow",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(6,3),model=rep('mlr',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==8 & model=='mlr') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Xsens",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(8,3),model=rep('mlr',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==9 & model=='mlr') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Xtal",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(9,3),model=rep('mlr',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

# Selecting best models por MA  using ANOVA -----------------------------------------
imu_results=results %>% filter(imuid==5 & model=='ma') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Gladiator",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(5,3),model=rep('ma',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==6 & model=='ma') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Gladiator",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(6,3),model=rep('ma',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==8 & model=='ma') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Gladiator",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(8,3),model=rep('ma',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd ))

imu_results=results %>% filter(imuid==9 & model=='ma') %>% select(timedelay,X1,X1_t_rsme,X2,X2_t_rsme,X6,X6_t_rsme)
best=select_n_taps(imu_results,"Gladiator",c("X1","X2","X6"))
best_models=rbind(best_models,data.frame(imuid=rep(9,3),model=rep('ma',3),sensor=paste("X",c(1,2,6),sep=""),timedelay=best$tap,rsme_mean=best$rsme_mean,rsme_sd=best$rsme_sd))


best_models$timedelay=as.integer(as.character(best_models$timedelay))
best_models$imuid=as.integer(as.character(best_models$imuid))
```


Either for TD-MLR and for MA models, the optimal size of $q$ should be determined. The strategy followed in this work consists of starting with a very short $q$ value and incrementing it by one period. For each new $q$ value the Root Square Mean Error (RMSE) of the resulting  model is calculated. For improving RSME estimate, a k-fold cross validation \cite{Mitchell1997} with $k=10$ is used to evaluate the models for every new lag term.  The process is repeated until $q$ reaches a predefined and  sufficiently large  value. Then, the optimal value for $q$ is determined by the model with the lower RSME.

In the case of MLP, not only $q$ but also the different number of hidden neurons $h$ are evaluated. In this case, a similar approach is followed for a subset of the of possible network typologies (i.e. $h$), again a k-fold cross validation with $k=10$ is used to evaluate the performance of every $q$ for each one of the selected $h$. Similarly to previous models, the model with the lower RMSE is used for determining the optimal values of $q$ and $h$  

Following standard machine learning methodology, the  models will be adjusted and evaluated using data from Stretch T1. Then, after the optimal lag length ($q$) for each model had been selected, an independent evaluation on stretch T2 is  conducted in order to get a better estimation of the error on unseen data.

### Results for the X1 (AccX) sensor  
The following table show the optimal $q$ value for  each model (MLR,MA and MLP) on the four selected IMUS. The $q$ values was obtained according to the strategy described in the previous section.
 
```{r}
best_models %>% filter(sensor=="X1") %>% arrange(imuid)
```

The plot shows the RSME distributions for sensor X1 provided by  TD-MLR, MA and MLP considering different values for $h$ (i.e. topologies)

```{r, fig.height=10, fig.width=10}
bw_data=inner_join(best_models %>% filter(sensor=="X1"),results, by=c("timedelay","imuid","model"))
bwplot(X1~model|imu,groups=timedelay,data=bw_data,scale='free'
       ,auto.key = F
       ,panel = panel.superpose
       ,panel.groups=function(x,y,group.number,...){
         xt <- x[x==min(x)] # find latest year
         yt <- y[y==min(y)] # find value at latest year 
         panel.text(x,yt,labels=levels(factor(bw_data$timedelay))[group.number],pos=1,...)
         panel.bwplot(x,y,...)
             },ylab="X1"
         )
```


### Results for the X2 (AccY) sensor  

The following table show the optimal $q$ value for  each model (MLR,MA and MLP) on the four selected IMUS. The $q$ values was obtained according to the strategy described in the previous section.

```{r}
best_models %>% filter(sensor=="X2") %>% arrange(imuid)
```

The plot shows the RSME distributions for sensor X2 provided by  TD-MLR, MA and MLP considering different values for $h$ (i.e. topologies)
```{r, fig.height=10, fig.width=10}
bw_data=inner_join(best_models %>% filter(sensor=="X2"),results, by=c("timedelay","imuid","model"))
bwplot(X2~model|imu,groups=timedelay,data=bw_data,scale='free'
       ,auto.key = F
       ,panel = panel.superpose
       ,panel.groups=function(x,y,group.number,...){
         xt <- x[x==min(x)] # find latest year
         yt <- y[y==min(y)] # find value at latest year 
         panel.text(x,yt,labels=levels(factor(bw_data$timedelay))[group.number],pos=1,...)
         panel.bwplot(x,y,...)
             },ylab="X2"
         )
```



### Results for the X6 (GiroZ) sensor  

The following table show the optimal $q$ value for  each model (MLR,MA and MLP) on the four selected IMUS. The $q$ values was obtained according to the strategy described in the previous section.
```{r}
best_models %>% filter(sensor=="X6") %>% arrange(imuid)
```

The plot shows the RSME distributions for sensor X6 provided by  TD-MLR, MA and MLP considering different values for $h$ (i.e. topologies)
```{r, fig.height=10, fig.width=10}
bw_data=inner_join(best_models %>% filter(sensor=="X6"),results, by=c("timedelay","imuid","model"))
bwplot(X6~model|imu,groups=timedelay,data=bw_data,scale='free'
       ,auto.key = F
       ,panel = panel.superpose
       ,panel.groups=function(x,y,group.number,...){
         xt <- x[x==min(x)] # find latest year
         yt <- y[y==min(y)] # find value at latest year 
         panel.text(x,yt,labels=levels(factor(bw_data$timedelay))[group.number],pos=1,...)
         panel.bwplot(x,y,...)

             },ylab="X6"
      
         )
```


The plot shows the RSME distributions for sensor X1 provided by  TD-MLR, MA and MLP considering different values for $h$ (i.e. topologies)

```{r best_taps, warning=FALSE}
## reshape results for selecting best models for ma and MLP
results_reduced=results %>% select(imuid,model,timedelay,X1,X2,X6)
results_reduced=melt(results_reduced,id.vars=c("imuid","model","timedelay"),variable.name="sensor",value.name="value")
best_taps=group_by(results_reduced,model,imuid,sensor,timedelay) %>% summarise(mean=mean(value),sd=sd(value)) %>% filter(mean==min(mean)) %>% arrange(desc(imuid)) %>% select(imuid,sensor,timedelay)
best_taps=reshape2::dcast(best_taps,imuid+model~sensor,value.var = 'timedelay')
```

### Final Models
In summary, after a 10-Fold Cross-validation the models with the lower RMSE are:

#### For MLP
```{r}

best_models %>% filter(grepl("mlp",model) & sensor=='X1')
best_models %>% filter(grepl("mlp",model) & sensor=='X2')
best_models %>% filter(grepl("mlp",model) & sensor=='X6')
```

#### For TD-MLR
```{r}
best_models %>% filter(grepl("mlr",model) & sensor=='X1')
best_models %>% filter(grepl("mlr",model) & sensor=='X2')
best_models %>% filter(grepl("mlr",model) & sensor=='X6')
```

#### For MA
```{r}
best_models %>% filter(grepl("ma",model) & sensor=='X1')
best_models %>% filter(grepl("ma",model) & sensor=='X2')
best_models %>% filter(grepl("ma",model) & sensor=='X6')


```

## Evaluation on T2 Stretch

The 100% of the training dataset (T1) is used for building the models and then evaluated on unseen data.
For performing the evaluation the model on unseen data, a sampling method is used on the  Stretch T2. In this case, a fixed window encompasing the 10% of the Stretch T2 is randomly and uniformly sampled with replacement. Such process is repeteated 40 times. 
```{r}
load("/home/harpo/Dropbox/shared/MEMS-ANN/results/results_samples_mlr_own_method.Rda")
```

```{r}

# Selecting best models for MA -----------------------------------------
ma_imus=list()
ma_imus[[5]]=(best_models %>% filter(imuid==5 & model=='ma'))$timedelay
ma_imus[[6]]=(best_models %>% filter(imuid==6 & model=='ma'))$timedelay
ma_imus[[8]]=(best_models %>% filter(imuid==8 & model=='ma'))$timedelay
ma_imus[[9]]=(best_models %>% filter(imuid==9 & model=='ma'))$timedelay

# Selecting best models for MLR 
mlr_imus=list()
mlr_imus[[5]]=(best_models %>% filter(imuid==5 & model=='mlr'))$timedelay
mlr_imus[[6]]=(best_models %>% filter(imuid==6 & model=='mlr'))$timedelay
mlr_imus[[8]]=(best_models %>% filter(imuid==8 & model=='mlr'))$timedelay
mlr_imus[[9]]=(best_models %>% filter(imuid==9 & model=='mlr'))$timedelay


# Selecting best models por MLP  using ANOVA
mlp_imus=list()

mlp_imus[[5]]=(best_models %>% filter(imuid==5 & grepl('mlp',model)))$timedelay
mlp_imus[[6]]=(best_models %>% filter(imuid==6 & grepl('mlp',model)))$timedelay
mlp_imus[[8]]=(best_models %>% filter(imuid==8 & grepl('mlp',model)))$timedelay
mlp_imus[[9]]=(best_models %>% filter(imuid==9 & grepl('mlp',model)))$timedelay

# Selecting the best MLP models
mlp_models=list()
mlp_models[[5]]=(best_models %>% filter(imuid==5 & grepl('mlp',model)))$model
mlp_models[[6]]=(best_models %>% filter(imuid==6 & grepl('mlp',model)))$model
mlp_models[[8]]=(best_models %>% filter(imuid==8 & grepl('mlp',model)))$model
mlp_models[[9]]=(best_models %>% filter(imuid==9 & grepl('mlp',model)))$model
```

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Setting up MLP parameters
mlp_arguments<-list(
      optimizer =  'rmsprop',
      hidden_node = 5, 
      num.round=200,
      array.layout = "rowmajor",
      out_activation = "rmse",
      device=mx.gpu(0),array.batch.size=12384,
      eval.metric = mx.metric.rmse, 
      activation = "tanh",
      out_node = 1,
      verbose=F
    )

# Performing the actual evaluation on testset
test_results=c()
for (imunum in selected_imus){
  # MLR
 test_results=rbind(test_results,cbind(model=rep('mlr'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlr_imus[[imunum]], model = 'lm', resamp=T,nresamp = 40,windowsize = 6000 , calcmean = T)))
  #MA  
  test_results=rbind(test_results,cbind(model=rep('ma'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = ma_imus[[imunum]], model = 'ma', resamp=T,nresamp = 40,windowsize = 6000, calcmean = T )))
  # MLP
  mlp_arguments$hidden_node=5
  test_results=rbind(test_results,cbind(model=rep('mlp5'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
  mlp_arguments$hidden_node=10
  test_results=rbind(test_results,cbind(model=rep('mlp10'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors =mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
  mlp_arguments$hidden_node=20
  test_results=rbind(test_results,cbind(model=rep('mlp20'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
  mlp_arguments$hidden_node=30
  test_results=rbind(test_results,cbind(model=rep('mlp30'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
  mlp_arguments$hidden_node=40
  test_results=rbind(test_results,cbind(model=rep('mlp40'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments , calcmean = T)))
  
  mlp_arguments$hidden_node=60
  test_results=rbind(test_results,cbind(model=rep('mlp60'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
  mlp_arguments$hidden_node=80
  test_results=rbind(test_results,cbind(model=rep('mlp80'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors =mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
  mlp_arguments$hidden_node=100
  test_results=rbind(test_results,cbind(model=rep('mlp100'),imuid=rep(imunum),exptraintest(imunum = imunum,taps_by_sensors = mlp_imus[[imunum]], model = 'mx.mlp', resamp=T,nresamp = 40,windowsize = 6000, model_arguments = mlp_arguments, calcmean = T )))
  
}
names(test_results)<-c("model","imuid","sample",
                       "timedelay_X1","X1","X1vsRaw","pred_meanX1","target_meanX1","original_meanX1",
                       "timedelay_X2","X2","X2vsRaw","pred_meanX2","target_meanX2","original_meanX2",
                       "timedelay_X6","X6","X6vsRaw","pred_meanX6","target_meanX6","original_meanX6")
save(test_results,file="/home/harpo/Dropbox/shared/MEMS-ANN/results/results_samples_mlr_own_method.Rda",compress = "xz")
```

### Comparing MLR vs Raw
This section analyze the results in terms of RMSE for the TD-MLR compared with the original RMSE.

```{r}
rmse_table<-c()
```

#### Evaluation on Gladiator
```{r}
glad_test_results=test_results %>% filter(imuid==5 &  model=='mlr') %>%select(imuid,X1,X1vsRaw,X2,X2vsRaw,X6,X6vsRaw)
names(glad_test_results)=c("imuid","X1_mlr","X1_raw","X2_mlr","X2_raw","X6_mlr","X6_raw")
glad_test_results=tidyr::gather(glad_test_results,"sensor","rmse",2:7)
glad_test_results=tidyr::separate(glad_test_results,sensor,c("sensor","model"),sep="_") 
px1=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="X1" & (model=="mlr"|model=="raw")))
px2=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="X2" & (model=="mlr"|model=="raw")))
px6=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="X6" & (model=="mlr"|model=="raw")))
```

The Figure below show the error distribution for the original Gladiator and the TD-MLR on the three sensors

```{r, echo=FALSE, message=FALSE, warning=FALSE}
glad_test_results[which(glad_test_results$sensor == "X1"),]$sensor="AccX"
glad_test_results[which(glad_test_results$sensor == "X2"),]$sensor="AccY"
glad_test_results[which(glad_test_results$sensor == "X6"),]$sensor="GyroZ"
glad_test_results[which(glad_test_results$model == "mlr"),]$model="td-mlr"
glad_test_results[which(glad_test_results$model == "raw"),]$model="non-comp."

rmse_table<-rbind(
  glad_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/glad-vs-mlr.pdf",width=5,height=3.5)
bwplot(rmse~model|sensor,data=glad_test_results,groups=model,layout=c(3,1), scales = list(y = list(relation="free",cex=1.2) ,x=list(draw=FALSE)),ylab="GLADIATOR\n(rmse)",do.out=F,col='skyblue')
dev.off()
#bwplot(rmse~model|sensor,data=glad_test_results,groups=model,layout=c(3,1),scales=list(relation='free'))
```


A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences are **statistically significant** for the three sensors


**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`


#### Evaluation on Xbow
```{r}
xbow_test_results=test_results %>% filter(imuid==6 &  model=='mlr') %>%select(imuid,X1,X1vsRaw,X2,X2vsRaw,X6,X6vsRaw)
names(xbow_test_results)=c("imuid","X1_mlr","X1_raw","X2_mlr","X2_raw","X6_mlr","X6_raw")
xbow_test_results=tidyr::gather(xbow_test_results,"sensor","rmse",2:7)
xbow_test_results=tidyr::separate(xbow_test_results,sensor,c("sensor","model"),sep="_") 
px1=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="X1" & (model=="mlr"|model=="raw")))
px2=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="X2" & (model=="mlr"|model=="raw")))
px6=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="X6" & (model=="mlr"|model=="raw")))
```

The Figure below show the error distribution for the original Xbow and the TD-MLR on the three sensors

```{r}

xbow_test_results[which(xbow_test_results$sensor == "X1"),]$sensor="AccX"
xbow_test_results[which(xbow_test_results$sensor == "X2"),]$sensor="AccY"
xbow_test_results[which(xbow_test_results$sensor == "X6"),]$sensor="GyroZ"
xbow_test_results[which(xbow_test_results$model == "mlr"),]$model="td-mlr"
xbow_test_results[which(xbow_test_results$model == "raw"),]$model="non-comp."

rmse_table<-rbind(rmse_table,
  xbow_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xbow-vs-mlr.pdf",width=5,height=3.5)
bwplot(rmse~model|sensor,data=xbow_test_results,groups=model,layout=c(3,1), strip=FALSE, scales = list(y = list(relation="free",cex=1.2),x=list(draw=FALSE)),color="red",ylab="CROSSBOW\n(rmse)",
       par.settings=list(box.rectangle=list(col="red",alpha=1),box.umbrella=list(col="red",alpha=1))
       
       )
dev.off()

#bwplot(rmse~model|sensor,data=xbow_test_results,groups=model,layout=c(3,1),scales=list(relation='free'))
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are statistically significant** for the three sensors


**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`


#### Evaluation on Xsens
```{r}
xsens_test_results=test_results %>% filter(imuid==8 &  model=='mlr') %>%select(imuid,X1,X1vsRaw,X2,X2vsRaw,X6,X6vsRaw)
names(xsens_test_results)=c("imuid","X1_mlr","X1_raw","X2_mlr","X2_raw","X6_mlr","X6_raw")
xsens_test_results=tidyr::gather(xsens_test_results,"sensor","rmse",2:7)
xsens_test_results=tidyr::separate(xsens_test_results,sensor,c("sensor","model"),sep="_") 
px1=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="X1" & (model=="mlr"|model=="raw")))
px2=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="X2" & (model=="mlr"|model=="raw")))
px6=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="X6" & (model=="mlr"|model=="raw")))
```

The Figure below show the error distribution for the original **Xsens** and the TD-MLR on the three sensors
```{r}
xsens_test_results[which(xsens_test_results$sensor == "X1"),]$sensor="AccX"
xsens_test_results[which(xsens_test_results$sensor == "X2"),]$sensor="AccY"
xsens_test_results[which(xsens_test_results$sensor == "X6"),]$sensor="GyroZ"
xsens_test_results[which(xsens_test_results$model == "mlr"),]$model="td-mlr"
xsens_test_results[which(xsens_test_results$model == "raw"),]$model="non-comp."

rmse_table<-rbind(rmse_table,
  xsens_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xsens-vs-mlr.pdf",width=5,height=3.5)
bwplot(rmse~model|sensor,data=xsens_test_results,groups=model,layout=c(3,1), strip=FALSE,scales = list(y = list(relation="free",cex=1.2),x=list(draw=FALSE)),color="green",ylab="XSENS\n(rmse)",
       par.settings=list(box.rectangle=list(col="green",alpha=1),box.umbrella=list(col="green",alpha=1))
       
       )
dev.off()
#bwplot(value~model|sensor,data=xsens_test_results,groups=model,layout=c(3,1),scales=list(relation='free'))
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on Xtal
```{r}
xtal_test_results=test_results %>% filter(imuid==9 &  model=='mlr') %>%select(imuid,X1,X1vsRaw,X2,X2vsRaw,X6,X6vsRaw)
names(xtal_test_results)=c("imuid","X1_mlr","X1_raw","X2_mlr","X2_raw","X6_mlr","X6_raw")
xtal_test_results=tidyr::gather(xtal_test_results,"sensor","rmse",2:7)
xtal_test_results=tidyr::separate(xtal_test_results,sensor,c("sensor","model"),sep="_") 
px1=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="X1" & (model=="mlr"|model=="raw")))
px2=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="X2" & (model=="mlr"|model=="raw")))
px6=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="X6" & (model=="mlr"|model=="raw")))
```

The Figure below show the error distribution for the original **Xtal** and the TD-MLR on the three sensors

```{r}
xtal_test_results[which(xtal_test_results$sensor == "X1"),]$sensor="AccX"
xtal_test_results[which(xtal_test_results$sensor == "X2"),]$sensor="AccY"
xtal_test_results[which(xtal_test_results$sensor == "X6"),]$sensor="GyroZ"
xtal_test_results[which(xtal_test_results$model == "mlr"),]$model="td-mlr"
xtal_test_results[which(xtal_test_results$model == "raw"),]$model="non-comp."

rmse_table<-rbind(rmse_table,
  xtal_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xtal-vs-mlr.pdf",width=5,height=4.5)
bwplot(rmse~model|sensor,data=xtal_test_results,groups=model,layout=c(3,1), strip=FALSE, scales = list(y = list(relation="free",cex=1.2),x = list(cex=1.2,rot=45)),color="black",ylab="XTAL\n(rmse)",
       par.settings=list(box.rectangle=list(col="black",alpha=1),box.umbrella=list(col="black",alpha=1))
       
       )
dev.off()
```


A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

### Comparing MLR vs MLP
The proposed method TD-MLR is now compared with an standard MLP on the four IMUS. Notice that model parameters result from Cross-validation on Stretch T1. For TD-MLR and MLP, the models  with the Lower RSME are selected.

#### Evaluation on Gladiator
```{r, include=FALSE}
#glad_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==5 & (model=='ma' | model=='mlr' | model=='mlp100'| model=='mlp5'))

glad_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==5 & 
  (   model=='mlr' | 
      model==as.character(mlp_models[[5]][1]) | 
      model==as.character(mlp_models[[5]][2]) |
      model==as.character(mlp_models[[5]][3]) 
  ))

glad_test_results=melt(glad_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

glad_test_results=glad_test_results %>% filter(model=='mlr' | 
  ( model==as.character(mlp_models[[5]][1]) & sensor=="X1") | 
  ( model==as.character(mlp_models[[5]][2]) & sensor=="X2") | 
  ( model==as.character(mlp_models[[5]][3]) & sensor=="X6")  
    )
glad_test_results$model=factor(glad_test_results$model, levels=c(levels(glad_test_results$model), "mlp","td-mlr"))
glad_test_results$sensor=factor(glad_test_results$sensor, levels=c(levels(glad_test_results$sensor), "AccX","AccY","GyroZ"))
# rename mlpxxx to mlp
glad_test_results[grepl("mlp",glad_test_results$model),]$model='mlp'
```

The Figure below show the error distribution for the TD-MLR and MLP for the three sensors on **Gladiator**

```{r}
glad_test_results[which(glad_test_results$sensor == "X1"),]$sensor="AccX"
glad_test_results[which(glad_test_results$sensor == "X2"),]$sensor="AccY"
glad_test_results[which(glad_test_results$sensor == "X6"),]$sensor="GyroZ"
glad_test_results[which(glad_test_results$model == "mlr"),]$model="td-mlr"

rmse_table<-rbind(rmse_table,
  glad_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/glad-mlr-vs-mlp.pdf",width=5,height=3.5)
bwplot(rmse~model|sensor,data=glad_test_results %>% filter((model=="mlp"|model=="td-mlr")),groups=factor(model),layout=c(3,1), scales = list(y = list(relation="free",cex=1.2) ,x=list(draw=FALSE)),ylab="GLADIATOR\n(rmse)",do.out=F,col='skyblue')
dev.off()

#bwplot(value~model|sensor,data=glad_test_results %>% filter((model=="mlp"|model=="mlr")),groups=factor(model),layout=c(3,1),scales=list(relation='free'),do.out = FALSE)
```

```{r}


px1=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="AccX" & (model=="mlp"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="AccY" & (model=="mlp"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="GyroZ" & (model=="mlp"|model=="td-mlr")))
```

A man-Whitney-Wilcox test is conducted for the three sensors and the results are presented in the following table
As can be observed the differences **are NOT statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on Xbow
```{r}
xbow_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==6 & 
  (   model=='mlr' | 
      model==as.character(mlp_models[[6]][1]) | 
      model==as.character(mlp_models[[6]][2]) |
      model==as.character(mlp_models[[6]][3]) 
  ))

xbow_test_results=melt(xbow_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

xbow_test_results=xbow_test_results %>% filter(model=='mlr' | 
  ( model==as.character(mlp_models[[6]][1]) & sensor=="X1") | 
  ( model==as.character(mlp_models[[6]][2]) & sensor=="X2") | 
  ( model==as.character(mlp_models[[6]][3]) & sensor=="X6")  
    )
xbow_test_results$model=factor(xbow_test_results$model, levels=c(levels(xbow_test_results$model), "mlp","td-mlr"))
xbow_test_results$sensor=factor(xbow_test_results$sensor, levels=c(levels(xbow_test_results$sensor), "AccX","AccY","GyroZ"))
# rename mlpxxx to mlp
xbow_test_results[grepl("mlp",xbow_test_results$model),]$model='mlp'
```

The Figure below show the error distribution for the TD-MLR and MLP for the three sensors on **Xbow**
```{r}
xbow_test_results[which(xbow_test_results$sensor == "X1"),]$sensor="AccX"
xbow_test_results[which(xbow_test_results$sensor == "X2"),]$sensor="AccY"
xbow_test_results[which(xbow_test_results$sensor == "X6"),]$sensor="GyroZ"
xbow_test_results[which(xbow_test_results$model == "mlr"),]$model="td-mlr"

rmse_table<-rbind(rmse_table,
  xbow_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xbow-mlr-vs-mlp.pdf",width=5,height=3.5)

bwplot(rmse~model|sensor,data=xbow_test_results %>% filter((model=="mlp"|model=="td-mlr")),groups=factor(model),layout=c(3,1),strip=FALSE, scales = list(y = list(relation="free",cex=1.2),x=list(draw=FALSE)),col="red",ylab="CROSSBOW\n(rmse)",
       par.settings=list(box.rectangle=list(col="red",alpha=1),box.umbrella=list(col="red",alpha=1)),do.out=F)
dev.off()



#bwplot(value~model|sensor,data=xbow_test_results %>% filter( model=="mlp"|model=="mlr") ,groups=factor(model),layout=c(3,1),scales=list(relation='free'))
```

```{r}
px1=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="AccX" & (model=="mlp"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="AccY" & (model=="mlp"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="GyroZ" & (model=="mlp"|model=="td-mlr")))
```
A man-Whitney-Wilcox test is conducted for the three sensors and the results are presented in the following table
As can be observed the differences **are NOT statistically significant** for the X1 and X2 sensors. However
in this case the difference for the X6 sensor are significant with a p.value <0.05

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on XSENS
```{r}
xsens_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==8 & 
  (   model=='mlr' | 
      model==as.character(mlp_models[[8]][1]) | 
      model==as.character(mlp_models[[8]][2]) |
      model==as.character(mlp_models[[8]][3]) 
  ))

xsens_test_results=melt(xsens_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

xsens_test_results=xsens_test_results %>% filter(model=='mlr' | 
  ( model==as.character(mlp_models[[8]][1]) & sensor=="X1") | 
  ( model==as.character(mlp_models[[8]][2]) & sensor=="X2") | 
  ( model==as.character(mlp_models[[8]][3]) & sensor=="X6") 
    )
xsens_test_results$model=factor(xsens_test_results$model, levels=c(levels(xsens_test_results$model), "mlp","td-mlr"))
xsens_test_results$sensor=factor(xsens_test_results$sensor, levels=c(levels(xsens_test_results$sensor), "AccX","AccY","GyroZ"))
# rename mlpxxx to mlp
xsens_test_results[grepl("mlp",xsens_test_results$model),]$model='mlp'
```

The Figure below show the error distribution for the TD-MLR and MLP for the three sensors on **Xsens**
```{r}
xsens_test_results[which(xsens_test_results$sensor == "X1"),]$sensor="AccX"
xsens_test_results[which(xsens_test_results$sensor == "X2"),]$sensor="AccY"
xsens_test_results[which(xsens_test_results$sensor == "X6"),]$sensor="GyroZ"
xsens_test_results[which(xsens_test_results$model == "mlr"),]$model="td-mlr"

rmse_table<-rbind(rmse_table,
  xsens_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xsens-mlr-vs-mlp.pdf",width=5,height=3.5)

bwplot(rmse~model|sensor,data=xsens_test_results %>% filter((model=="mlp"|model=="td-mlr")),groups=factor(model),layout=c(3,1),strip=FALSE, scales = list(y = list(relation="free",cex=1.2),x=list(draw=FALSE)),col="green",ylab="XSENS\n(rmse)",
       par.settings=list(box.rectangle=list(col="green",alpha=1),box.umbrella=list(col="green",alpha=1)),do.out=F)
dev.off()



#bwplot(value~model|sensor,data=xsens_test_results %>% filter( model=="mlp"|model=="mlr") ,groups=factor(model),layout=c(3,1),scales=list(relation='free'))
```

```{r}
px1=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="AccX" & (model=="mlp"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="AccY" & (model=="mlp"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="GyroZ" & (model=="mlp"|model=="td-mlr")))
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are NOT statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on Xtal
```{r}
xtal_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==9 & 
  (   model=='mlr' | 
      model==as.character(mlp_models[[9]][1]) | 
      model==as.character(mlp_models[[9]][2]) |
      model==as.character(mlp_models[[9]][3]) 
  ))

xtal_test_results=melt(xtal_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

xtal_test_results=xtal_test_results %>% filter(model=='mlr' | 
  ( model==as.character(mlp_models[[9]][1]) & sensor=="X1") | 
  ( model==as.character(mlp_models[[9]][2]) & sensor=="X2") | 
  ( model==as.character(mlp_models[[9]][3]) & sensor=="X6") 
    )
xtal_test_results$model=factor(xtal_test_results$model, levels=c(levels(xtal_test_results$model), "mlp","td-mlr"))
xtal_test_results$sensor=factor(xtal_test_results$sensor, levels=c(levels(xtal_test_results$sensor), "AccX","AccY","GyroZ"))
# rename mlpxxx to mlp
xtal_test_results[grepl("mlp",xtal_test_results$model),]$model='mlp'
```

The Figure below show the error distribution for the TD-MLR and MLP for the three sensors on **Xtal**
```{r}
xtal_test_results[which(xtal_test_results$sensor == "X1"),]$sensor="AccX"
xtal_test_results[which(xtal_test_results$sensor == "X2"),]$sensor="AccY"
xtal_test_results[which(xtal_test_results$sensor == "X6"),]$sensor="GyroZ"
xtal_test_results[which(xtal_test_results$model == "mlr"),]$model="td-mlr"

rmse_table<-rbind(rmse_table,
  xtal_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)
pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xtal-mlr-vs-mlp.pdf",width=5,height=4.5)
bwplot(rmse~model|sensor,data=xtal_test_results %>% filter((model=="mlp"|model=="td-mlr")),groups=factor(model),layout=c(3,1),strip=FALSE, 
       scales = list(y = list(relation="free",cex=1.2),
                     x = list(cex=1.2,rot=45,labels=c("            mlp","td-mlr"))),
       col="black",ylab="XTAL\n(rmse)",
       par.settings=list(box.rectangle=list(col="black",alpha=1),box.umbrella=list(col="black",alpha=1)),do.out=F)
dev.off()

#bwplot(value~model|sensor,data=xtal_test_results %>% filter( model=="mlp"|model=="mlr") ,groups=factor(model),layout=c(3,1),scales=list(relation='free'))
```

```{r}
px1=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="AccX" & (model=="mlp"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="AccY" & (model=="mlp"|model=="td-mlr")))
px3=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="GyroZ" & (model=="mlp"|model=="td-mlr")))
```
A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are NOT statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Discussion

After evaluating MLR and MLP, it is possible to conclude that both model performances are comparable in terms of RMSE for the four IMUs evaluated.
MLR has shown an equivalent performance in most of the the cases yet outperforming in the case of sensor X6 (GyroZ) for the Crossbow(xbow). 

### Comparing MLR vs MA

In this section the proposed MLR model is compared with the standar MA method for filtering noise.

#### Evaluation on Gladiator
```{r}
glad_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==5 & (model=='ma' | model=='mlr'))
glad_test_results=melt(glad_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

glad_test_results$model=factor(glad_test_results$model, levels=c(levels(glad_test_results$model), "mlp","td-mlr"))
glad_test_results$sensor=factor(glad_test_results$sensor, levels=c(levels(glad_test_results$sensor), "AccX","AccY","GyroZ"))


```

The Figure below show the error distribution for the TD-MLR and MA for the three sensors on **Gladiator**
```{r}
glad_test_results[which(glad_test_results$sensor == "X1"),]$sensor="AccX"
glad_test_results[which(glad_test_results$sensor == "X2"),]$sensor="AccY"
glad_test_results[which(glad_test_results$sensor == "X6"),]$sensor="GyroZ"
glad_test_results[which(glad_test_results$model == "mlr"),]$model="td-mlr"
rmse_table<-rbind(rmse_table,
  glad_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)
pdf("~/Dropbox/shared/MEMS-ANN/results/figs/glad-mlr-vs-ma.pdf",width=5,height=3.5)
bwplot(rmse~model|sensor,data=glad_test_results %>% filter((model=="ma"|model=="td-mlr")),groups=factor(model),layout=c(3,1), scales = list(y = list(relation="free",cex=1.2) ,x=list(draw=FALSE)),ylab="GLADIATOR\n(rmse)",do.out=F,col='skyblue')
dev.off()


#bwplot(value~model|sensor,data=glad_test_results %>% filter((model=="ma"|model=="mlr")),groups=factor(model),layout=c(3,1),scales=list(relation='free'))


px1=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="AccX" & (model=="ma"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="AccY" & (model=="ma"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=glad_test_results %>% filter(sensor=="GyroZ" & (model=="ma"|model=="td-mlr")))
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are NOT statistically significant** for the two out of the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on Xbow
```{r}
xbow_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==6 & (model=='ma' | model=='mlr'))
xbow_test_results=melt(xbow_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

xbow_test_results$model=factor(xbow_test_results$model, levels=c(levels(xbow_test_results$model), "mlp","td-mlr"))
xbow_test_results$sensor=factor(xbow_test_results$sensor, levels=c(levels(xbow_test_results$sensor), "AccX","AccY","GyroZ"))
```

The Figure below show the error distribution for the TD-MLR and MA for the three sensors on **Xbow**

```{r}
xbow_test_results[which(xbow_test_results$sensor == "X1"),]$sensor="AccX"
xbow_test_results[which(xbow_test_results$sensor == "X2"),]$sensor="AccY"
xbow_test_results[which(xbow_test_results$sensor == "X6"),]$sensor="GyroZ"
xbow_test_results[which(xbow_test_results$model == "mlr"),]$model="td-mlr"

rmse_table<-rbind(rmse_table,
  xbow_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)
pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xbow-mlr-vs-ma.pdf",width=5,height=3.5)
bwplot(rmse~model|sensor,data=xbow_test_results %>% filter((model=="ma"|model=="td-mlr")),groups=factor(model),layout=c(3,1),strip=FALSE, scales = list(y = list(relation="free",cex=1.2),x=list(draw=FALSE)),col="red",ylab="CROSSBOW\n(rmse)",
       par.settings=list(box.rectangle=list(col="red",alpha=1),box.umbrella=list(col="red",alpha=1)),do.out=F)
dev.off()

#bwplot(value~model|sensor,data=xbow_test_results %>% filter( model=="ma"|model=="mlr") ,groups=factor(model),layout=c(3,1),scales=list(relation='free'))
px1=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="AccX" & (model=="ma"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="AccY" & (model=="ma"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=xbow_test_results %>% filter(sensor=="GyroZ" & (model=="ma"|model=="td-mlr")))
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on XSENS
```{r}
xsens_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==8 & (model=='ma' | model=='mlr'))
xsens_test_results=melt(xsens_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

xsens_test_results$model=factor(xsens_test_results$model, levels=c(levels(xsens_test_results$model), "mlp","td-mlr"))
xsens_test_results$sensor=factor(xsens_test_results$sensor, levels=c(levels(xsens_test_results$sensor), "AccX","AccY","GyroZ"))
```

The Figure below show the error distribution for the TD-MLR and MA for the three sensors on **Xsens**

```{r}

xsens_test_results[which(xsens_test_results$sensor == "X1"),]$sensor="AccX"
xsens_test_results[which(xsens_test_results$sensor == "X2"),]$sensor="AccY"
xsens_test_results[which(xsens_test_results$sensor == "X6"),]$sensor="GyroZ"
xsens_test_results[which(xsens_test_results$model == "mlr"),]$model="td-mlr"
rmse_table<-rbind(rmse_table,
  xsens_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)
pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xsens-mlr-vs-ma.pdf",width=5,height=3.5)

bwplot(rmse~model|sensor,data=xsens_test_results %>% filter((model=="ma"|model=="td-mlr")),groups=factor(model),layout=c(3,1),strip=FALSE, scales = list(y = list(relation="free",cex=1.2),x=list(draw=FALSE)),col="green",ylab="XSENS\n(rmse)",
       par.settings=list(box.rectangle=list(col="green",alpha=1),box.umbrella=list(col="green",alpha=1)),do.out=F)
dev.off()


#bwplot(value~model|sensor,data=xsens_test_results %>% filter( model=="ma"|model=="mlr") ,groups=factor(model),layout=c(3,1),scales=list(relation='free'))
px1=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="AccX" & (model=="ma"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="AccY" & (model=="ma"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=xsens_test_results %>% filter(sensor=="GyroZ" & (model=="ma"|model=="td-mlr")))
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Evaluation on Xtal
```{r}
xtal_test_results=test_results %>% select(model,imuid,sample,X1,X2,X6) %>% filter(imuid==9 & (model=='ma' | model=='mlr'))
xtal_test_results=melt(xtal_test_results,id.vars=c("imuid","model","sample"),variable.name="sensor",value.name="rmse")

xtal_test_results$model=factor(xtal_test_results$model, levels=c(levels(xtal_test_results$model), "mlp","td-mlr"))
xtal_test_results$sensor=factor(xtal_test_results$sensor, levels=c(levels(xtal_test_results$sensor), "AccX","AccY","GyroZ"))
```

The Figure below show the error distribution for the TD-MLR and MA for the three sensors on **Xtal**
```{r}
xtal_test_results[which(xtal_test_results$sensor == "X1"),]$sensor="AccX"
xtal_test_results[which(xtal_test_results$sensor == "X2"),]$sensor="AccY"
xtal_test_results[which(xtal_test_results$sensor == "X6"),]$sensor="GyroZ"
xtal_test_results[which(xtal_test_results$model == "mlr"),]$model="td-mlr"
rmse_table<-rbind(rmse_table,
  xtal_test_results%>% group_by(model,imuid,sensor)%>% summarise(mean=mean(rmse),sd=sd(rmse))
)

pdf("~/Dropbox/shared/MEMS-ANN/results/figs/xtal-mlr-vs-ma.pdf",width=5,height=4.5)
bwplot(rmse~model|sensor,data=xtal_test_results %>% filter((model=="ma"|model=="td-mlr")),groups=factor(model),layout=c(3,1),strip=FALSE, 
       scales = list(y = list(relation="free",cex=1.2),x = list(cex=1.2,rot=45,labels=c("             ma","td-mlr"))),
       col="black",ylab="XTAL\n(rmse)", par.settings=list(box.rectangle=list(col="black",alpha=1),box.umbrella=list(col="black",alpha=1)),do.out=F)
dev.off()

#bwplot(value~model|sensor,data=xtal_test_results %>% filter( model=="ma"|model=="mlr") ,groups=factor(model),layout=c(3,1),scales=list(relation='free'))
px1=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="AccX" & (model=="ma"|model=="td-mlr")))
px2=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="AccY" & (model=="ma"|model=="td-mlr")))
px6=wilcox.test(rmse~model,data=xtal_test_results %>% filter(sensor=="GyroZ" & (model=="ma"|model=="td-mlr")))



#rmse_table %>%unique() %>% arrange(model)
#%>% group_by(model,imuid) %>% summarise(n=n())

rmse_table
rmse_table %>% unique()
ggplot(rmse_table %>% unique())+
  geom_col(aes(x=model,y=mean,fill=model),position=position_dodge())+
  geom_errorbar(aes(x=model,ymin= mean-sd, ymax=mean+sd),position=position_dodge(),color='black', width=0.3,size=.3)+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_x_discrete(limits=c('td-mlr','mlp','ma','non-comp.'))+
  facet_wrap(~sensor+imuid, scales = "free_y",strip.position = "bottom")
```

A man-Whitney-Wilcox test is conducted for the three sensors and results are presented in the following table
As can be observed the differences **are statistically significant** for the three sensors

**Sensor Name**  |      **X1**     | **X2**          | **X6**
---------------- |  -------------- | --------------- | ---------------
  **P Value**    | `r px1$p.value` | `r px2$p.value` | `r px6$p.value`

#### Discussion

In general the TD-MLR outperforms in three out of four IMUS. Namely, Xbow, Xsens and Xtal, while in the case of Gladiator, the performance of TD-MLR has not show a statistically significant difference.
Clearly, given the simplicity of the MA method, it is fair to say that for the Gladiator IMU the application of a TD-MLR model is not justified. However, the MLR has shown a significant difference in the remaining IMUs.

# Concluding Remarks

* Despite the claims of non-linearity on MEMS sensors, it has been verified that a simple linear method such as TD-MLR can reduce significantly the error on the four evaluated IMUs. The simplicity behind TD-MLR facilitates its deployment on an embedded system which combines the learned filter and the MEMS sensors. The resulting system could provide a significant improvement on the development of UAV using low-cost techonology.
* Non-linear MLP shows a similar performance compared with MLR, with the inconvenience of incrementing the complexity of the model. A situation that should be considered for further deployment on embedded systems.
* On MEMS sensors with a high noise level, a traditional and simple method such as MA shows a performance comparable to MLR. It seems that a higher noise level turns things harder for learning nav-grade IMU behavior using TD-MLR. Maybe if more data had been available the algorithm would eventually outperforms MA. 
